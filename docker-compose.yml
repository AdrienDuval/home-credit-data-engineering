services:

  # =========================
  # PostgreSQL (Source A)
  # =========================
  postgres:
    image: postgres:15
    container_name: home_credit_postgres
    restart: always
    env_file: .env
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    ports:
      - "${POSTGRES_PORT}:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./docker/postgres/init:/docker-entrypoint-initdb.d
      - ./data/application:/data/application
    networks:
      - home_credit_net

  # =========================
  # HDFS NameNode
  # =========================
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: home_credit_namenode
    restart: always
    environment:
      - CLUSTER_NAME=home_credit_cluster
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    ports:
      - "9870:9870"
      - "8020:8020"
    volumes:
      - namenode_data:/hadoop/dfs/name
    networks:
      - home_credit_net

  # =========================
  # HDFS DataNode
  # =========================
  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: home_credit_datanode
    restart: always
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - SERVICE_PRECONDITION=namenode:9870
    ports:
      - "9864:9864"
    volumes:
      - datanode_data:/hadoop/dfs/data
    depends_on:
      - namenode
    networks:
      - home_credit_net

  # =========================
  # YARN ResourceManager (cluster resource manager; use with --master yarn)
  # =========================
  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    container_name: home_credit_resourcemanager
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9870 datanode:9864"
    env_file:
      - ./docker/hadoop.env
    ports:
      - "8088:8088"
    networks:
      - home_credit_net
    depends_on:
      - namenode
      - datanode

  # =========================
  # YARN NodeManager (runs containers for Spark executors)
  # =========================
  nodemanager:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    container_name: home_credit_nodemanager
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9870 datanode:9864 resourcemanager:8088"
    env_file:
      - ./docker/hadoop.env
    ports:
      - "8042:8042"
    volumes:
      # So YARN executors can read local CSV paths (file:///opt/spark/work-dir/data/...)
      - ./data:/opt/spark/work-dir/data
    networks:
      - home_credit_net
    depends_on:
      - namenode
      - datanode
      - resourcemanager

  # =========================
  # Hive Metastore (catalog for Silver/Gold tables on HDFS)
  # =========================
  hive-metastore:
    image: apache/hive:3.1.3
    container_name: home_credit_hive_metastore
    restart: always
    environment:
      SERVICE_NAME: metastore
      DB_DRIVER: postgres
      SERVICE_OPTS: >-
        -Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver
        -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres:5432/metastore_db
        -Djavax.jdo.option.ConnectionUserName=hive
        -Djavax.jdo.option.ConnectionPassword=hive
        -Dhive.metastore.warehouse.dir=hdfs://namenode:8020/user/hive/warehouse
        -Dfs.defaultFS=hdfs://namenode:8020
    ports:
      - "9083:9083"
    depends_on:
      - namenode
      - postgres
    networks:
      - home_credit_net

  # =========================
  # Spark Master (OFFICIAL)
  # =========================
  spark-master:
    image: apache/spark:3.5.0
    container_name: home_credit_spark_master
    restart: always
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      - HADOOP_CONF_DIR=/opt/spark/work-dir/docker/hadoop-conf
    networks:
      - home_credit_net
    volumes:
      - ./:/opt/spark/work-dir
      - ./jars/postgresql-42.7.9.jar:/opt/spark/jars/postgresql.jar
      - ./docker/hadoop-conf:/opt/spark/work-dir/docker/hadoop-conf

  # =========================
  # Spark Worker (OFFICIAL)
  # =========================
  spark-worker:
    image: apache/spark:3.5.0
    container_name: home_credit_spark_worker
    restart: always
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      - HADOOP_CONF_DIR=/opt/spark/work-dir/docker/hadoop-conf
    depends_on:
      - spark-master
    networks:
      - home_credit_net
    volumes:
      - ./:/opt/spark/work-dir
      - ./docker/hadoop-conf:/opt/spark/work-dir/docker/hadoop-conf

volumes:
  postgres_data:
  namenode_data:
  datanode_data:

networks:
  home_credit_net:
    driver: bridge
